{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a09c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47944031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8889c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"OTUS\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c053e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/ubuntu/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3a023b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 13:07:04,417 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, overwrite=false, append=false, useDiff=false, useRdiff=false, fromSnapshot=null, toSnapshot=null, skipCRC=false, blocking=true, numListstatusThreads=0, maxMaps=20, mapBandwidth=0.0, copyStrategy='uniformsize', preserveStatus=[], atomicWorkPath=null, logPath=null, sourceFileListing=null, sourcePaths=[s3a://otus-mlops-source-data/2019-08-22.txt], targetPath=/user/ubuntu/data/2019-08-22.txt, filtersFile='null', blocksPerChunk=0, copyBufferSize=8192, verboseLog=false, directWrite=false}, sourcePaths=[s3a://otus-mlops-source-data/2019-08-22.txt], targetPathExists=false, preserveRawXattrsfalse\n",
      "2025-12-10 13:07:04,526 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-12-10 13:07:04,669 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-12-10 13:07:04,669 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2025-12-10 13:07:04,842 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-12-10 13:07:04,844 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-12-10 13:07:04,844 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2025-12-10 13:07:07,601 INFO tools.SimpleCopyListing: Paths (files+dirs) cnt = 1; dirCnt = 0\n",
      "2025-12-10 13:07:07,601 INFO tools.SimpleCopyListing: Build file listing completed.\n",
      "2025-12-10 13:07:07,603 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb\n",
      "2025-12-10 13:07:07,603 INFO Configuration.deprecation: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor\n",
      "2025-12-10 13:07:07,641 INFO tools.DistCp: Number of paths in the copy list: 1\n",
      "2025-12-10 13:07:07,646 INFO tools.DistCp: Number of paths in the copy list: 1\n",
      "2025-12-10 13:07:07,651 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2025-12-10 13:07:07,721 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2025-12-10 13:07:07,907 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1478760118_0001\n",
      "2025-12-10 13:07:07,907 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-10 13:07:08,076 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2025-12-10 13:07:08,076 INFO tools.DistCp: DistCp job-id: job_local1478760118_0001\n",
      "2025-12-10 13:07:08,077 INFO mapreduce.Job: Running job: job_local1478760118_0001\n",
      "2025-12-10 13:07:08,079 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2025-12-10 13:07:08,085 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2025-12-10 13:07:08,085 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2025-12-10 13:07:08,086 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.tools.mapred.CopyCommitter\n",
      "2025-12-10 13:07:08,131 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2025-12-10 13:07:08,132 INFO mapred.LocalJobRunner: Starting task: attempt_local1478760118_0001_m_000000_0\n",
      "2025-12-10 13:07:08,176 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2025-12-10 13:07:08,176 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2025-12-10 13:07:08,209 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2025-12-10 13:07:08,212 INFO mapred.MapTask: Processing split: file:/tmp/hadoop/mapred/staging/ubuntu212294871/.staging/_distcp-399668942/fileList.seq:0+223\n",
      "2025-12-10 13:07:08,219 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2025-12-10 13:07:08,220 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2025-12-10 13:07:08,233 INFO mapred.CopyMapper: Copying s3a://otus-mlops-source-data/2019-08-22.txt to hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt\n",
      "2025-12-10 13:07:08,246 INFO mapred.RetriableFileCopyCommand: Copying s3a://otus-mlops-source-data/2019-08-22.txt to hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt\n",
      "2025-12-10 13:07:08,248 INFO mapred.RetriableFileCopyCommand: Creating temp file: hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1478760118_0001_m_000000_0.1765372028246\n",
      "2025-12-10 13:07:08,248 INFO mapred.RetriableFileCopyCommand: Writing to temporary target file path hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1478760118_0001_m_000000_0.1765372028246\n",
      "2025-12-10 13:07:09,082 INFO mapreduce.Job: Job job_local1478760118_0001 running in uber mode : false\n",
      "2025-12-10 13:07:09,083 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-10 13:07:20,197 INFO mapred.LocalJobRunner: 24.6% Copying s3a://otus-mlops-source-data/2019-08-22.txt to hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt [659.8M/2.6G] > map\n",
      "2025-12-10 13:07:21,099 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-10 13:07:26,198 INFO mapred.LocalJobRunner: 39.7% Copying s3a://otus-mlops-source-data/2019-08-22.txt to hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt [1.0G/2.6G] > map\n",
      "2025-12-10 13:07:32,200 INFO mapred.LocalJobRunner: 52.6% Copying s3a://otus-mlops-source-data/2019-08-22.txt to hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt [1.4G/2.6G] > map\n",
      "2025-12-10 13:07:38,202 INFO mapred.LocalJobRunner: 67.3% Copying s3a://otus-mlops-source-data/2019-08-22.txt to hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt [1.8G/2.6G] > map\n",
      "2025-12-10 13:07:44,204 INFO mapred.LocalJobRunner: 81.1% Copying s3a://otus-mlops-source-data/2019-08-22.txt to hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt [2.1G/2.6G] > map\n",
      "2025-12-10 13:07:50,208 INFO mapred.LocalJobRunner: 96.8% Copying s3a://otus-mlops-source-data/2019-08-22.txt to hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt [2.5G/2.6G] > map\n",
      "2025-12-10 13:07:51,206 INFO mapred.RetriableFileCopyCommand: Renaming temporary target file path hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1478760118_0001_m_000000_0.1765372028246 to hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt\n",
      "2025-12-10 13:07:51,215 INFO mapred.RetriableFileCopyCommand: Completed writing hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt (2807409271 bytes)\n",
      "2025-12-10 13:07:51,223 INFO mapred.LocalJobRunner: 96.8% Copying s3a://otus-mlops-source-data/2019-08-22.txt to hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt [2.5G/2.6G] > map\n",
      "2025-12-10 13:07:51,229 INFO mapred.Task: Task:attempt_local1478760118_0001_m_000000_0 is done. And is in the process of committing\n",
      "2025-12-10 13:07:51,232 INFO mapred.LocalJobRunner: 96.8% Copying s3a://otus-mlops-source-data/2019-08-22.txt to hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt [2.5G/2.6G] > map\n",
      "2025-12-10 13:07:51,234 INFO mapred.Task: Task attempt_local1478760118_0001_m_000000_0 is allowed to commit now\n",
      "2025-12-10 13:07:51,236 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1478760118_0001_m_000000_0' to file:/tmp/hadoop/mapred/staging/ubuntu212294871/.staging/_distcp-399668942/_logs\n",
      "2025-12-10 13:07:51,237 INFO mapred.LocalJobRunner: 100.0% Copying s3a://otus-mlops-source-data/2019-08-22.txt to hdfs://rc1a-dataproc-m-tsbfqjbg22viqoba.mdb.yandexcloud.net/user/ubuntu/data/2019-08-22.txt [2.6G/2.6G]\n",
      "2025-12-10 13:07:51,238 INFO mapred.Task: Task 'attempt_local1478760118_0001_m_000000_0' done.\n",
      "2025-12-10 13:07:51,247 INFO mapred.Task: Final Counters for attempt_local1478760118_0001_m_000000_0: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=187174\n",
      "\t\tFILE: Number of bytes written=748055\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=0\n",
      "\t\tHDFS: Number of bytes written=2807409271\n",
      "\t\tHDFS: Number of read operations=6\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tS3A: Number of bytes read=2807409271\n",
      "\t\tS3A: Number of bytes written=0\n",
      "\t\tS3A: Number of read operations=6\n",
      "\t\tS3A: Number of large read operations=0\n",
      "\t\tS3A: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=0\n",
      "\t\tInput split bytes=152\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=142\n",
      "\t\tTotal committed heap usage (bytes)=666370048\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=255\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8\n",
      "\tDistCp Counters\n",
      "\t\tBandwidth in Btyes=66843077\n",
      "\t\tBytes Copied=2807409271\n",
      "\t\tBytes Expected=2807409271\n",
      "\t\tFiles Copied=1\n",
      "2025-12-10 13:07:51,248 INFO mapred.LocalJobRunner: Finishing task: attempt_local1478760118_0001_m_000000_0\n",
      "2025-12-10 13:07:51,249 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2025-12-10 13:07:51,269 INFO mapred.CopyCommitter: Cleaning up temporary work folder: file:/tmp/hadoop/mapred/staging/ubuntu212294871/.staging/_distcp-399668942\n",
      "2025-12-10 13:07:52,113 INFO mapreduce.Job: Job job_local1478760118_0001 completed successfully\n",
      "2025-12-10 13:07:52,125 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=187174\n",
      "\t\tFILE: Number of bytes written=748055\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=0\n",
      "\t\tHDFS: Number of bytes written=2807409271\n",
      "\t\tHDFS: Number of read operations=6\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tS3A: Number of bytes read=2807409271\n",
      "\t\tS3A: Number of bytes written=0\n",
      "\t\tS3A: Number of read operations=6\n",
      "\t\tS3A: Number of large read operations=0\n",
      "\t\tS3A: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=0\n",
      "\t\tInput split bytes=152\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=142\n",
      "\t\tTotal committed heap usage (bytes)=666370048\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=255\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8\n",
      "\tDistCp Counters\n",
      "\t\tBandwidth in Btyes=66843077\n",
      "\t\tBytes Copied=2807409271\n",
      "\t\tBytes Expected=2807409271\n",
      "\t\tFiles Copied=1\n",
      "2025-12-10 13:07:52,127 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2025-12-10 13:07:52,127 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2025-12-10 13:07:52,127 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "!hadoop distcp s3a://otus-mlops-source-data/2019-08-22.txt /user/ubuntu/data/2019-08-22.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d012a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 ubuntu hadoop 2807409271 2025-12-10 13:07 data/2019-08-22.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4a03dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 46,988,418 —Å—Ç—Ä–æ–∫\n"
     ]
    }
   ],
   "source": [
    "#bucket = \"oa-otus-bucket-b1gfn58hulkq64deu28q\"\n",
    "#file_path = f\"s3a://{bucket}/2019-08-22.txt\"\n",
    "file_path = f\"data/2019-08-22.txt\"\n",
    "\n",
    "# –ß–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É\n",
    "lines = spark.sparkContext.textFile(file_path)\n",
    "indexed = lines.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "\n",
    "# üî• –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É –≤ –∫–æ—Ä—Ç–µ–∂ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!)\n",
    "rdd_of_tuples = indexed.map(lambda line: (line,))\n",
    "\n",
    "df_clean = spark.createDataFrame(rdd_of_tuples, [\"value\"])\n",
    "\n",
    "# –ü–∞—Ä—Å–∏–º\n",
    "df = df_clean.select(split(col(\"value\"), \",\").alias(\"cols\")).select(\n",
    "    col(\"cols\")[0].cast(\"long\").alias(\"tranaction_id\"),\n",
    "    col(\"cols\")[1].cast(\"string\").alias(\"tx_datetime\"),\n",
    "    col(\"cols\")[2].cast(\"long\").alias(\"customer_id\"),\n",
    "    col(\"cols\")[3].cast(\"long\").alias(\"terminal_id\"),\n",
    "    col(\"cols\")[4].cast(\"double\").alias(\"tx_amount\"),\n",
    "    col(\"cols\")[5].cast(\"long\").alias(\"tx_time_seconds\"),\n",
    "    col(\"cols\")[6].cast(\"long\").alias(\"tx_time_days\"),\n",
    "    col(\"cols\")[7].cast(\"int\").alias(\"tx_fraud\"),\n",
    "    col(\"cols\")[8].cast(\"int\").alias(\"tx_fraud_scenario\")\n",
    ")\n",
    "\n",
    "df.cache()\n",
    "print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {df.count():,} —Å—Ç—Ä–æ–∫\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b99838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "\n",
      "‚ùó –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–ø–æ—Å–ª–µ cast):\n",
      "\n",
      "‚ùó –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è:\n",
      "  ‚Ä¢ tx_amount <= 0: 884\n",
      "  ‚Ä¢ –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞—Ç—ã: 100\n",
      "  ‚Ä¢ –î—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è ID —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: 181\n",
      "\n",
      "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n",
      "  –í—Å–µ–≥–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: 46,988,418\n",
      "  –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö: 2,527,005\n",
      "  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: 988,545\n",
      "  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤: 1,006\n",
      "  –°—É–º–º–∞: –æ—Ç 0.00 –¥–æ 3773.34\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...\\n\")\n",
    "\n",
    "# 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NULL (—Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è —Ç–∏–ø–æ–≤)\n",
    "null_counts = df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns\n",
    "]).collect()[0]\n",
    "\n",
    "print(\"‚ùó –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–ø–æ—Å–ª–µ cast):\")\n",
    "for col_name in df.columns:\n",
    "    cnt = null_counts[col_name]\n",
    "    if cnt > 0:\n",
    "        print(f\"  ‚Ä¢ {col_name}: {cnt:,}\")\n",
    "\n",
    "# 2. –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "checks = {\n",
    "    \"tranaction_id < 0\": df.filter(col(\"tranaction_id\") < 0).count(),\n",
    "    \"tx_amount <= 0\": df.filter(col(\"tx_amount\") <= 0).count(),\n",
    "    \"tx_time_seconds < 0\": df.filter(col(\"tx_time_seconds\") < 0).count(),\n",
    "    \"tx_time_days < 0\": df.filter(col(\"tx_time_days\") < 0).count(),\n",
    "    \"tx_fraud not in (0,1)\": df.filter(~col(\"tx_fraud\").isin([0, 1])).count(),\n",
    "    \"tx_fraud_scenario < 0\": df.filter(col(\"tx_fraud_scenario\") < 0).count(),\n",
    "}\n",
    "\n",
    "print(\"\\n‚ùó –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è:\")\n",
    "for desc, cnt in checks.items():\n",
    "    if cnt > 0:\n",
    "        print(f\"  ‚Ä¢ {desc}: {cnt:,}\")\n",
    "\n",
    "# 3. –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞—Ç—ã\n",
    "df_parsed = df.withColumn(\n",
    "    \"parsed_dt\",\n",
    "    to_timestamp(col(\"tx_datetime\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "invalid_dates = df_parsed.filter(col(\"parsed_dt\").isNull()).count()\n",
    "if invalid_dates > 0:\n",
    "    print(f\"  ‚Ä¢ –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞—Ç—ã: {invalid_dates:,}\")\n",
    "\n",
    "# 4. –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ: –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ –±–µ–∑ —Å—Ü–µ–Ω–∞—Ä–∏—è\n",
    "bad_fraud = df.filter((col(\"tx_fraud\") == 1) & (col(\"tx_fraud_scenario\") == 0)).count()\n",
    "if bad_fraud > 0:\n",
    "    print(f\"  ‚Ä¢ tx_fraud=1, –Ω–æ tx_fraud_scenario=0: {bad_fraud:,}\")\n",
    "\n",
    "# 5. –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ tranaction_id\n",
    "duplicates = df.groupBy(\"tranaction_id\").count().filter(col(\"count\") > 1).count()\n",
    "if duplicates > 0:\n",
    "    print(f\"  ‚Ä¢ –î—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è ID —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {duplicates:,}\")\n",
    "\n",
    "# 6. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "stats = df.agg(\n",
    "    sum(\"tx_fraud\").alias(\"fraud_count\"),\n",
    "    countDistinct(\"customer_id\").alias(\"customers\"),\n",
    "    countDistinct(\"terminal_id\").alias(\"terminals\"),\n",
    "    min(\"tx_amount\").alias(\"min_amt\"),\n",
    "    max(\"tx_amount\").alias(\"max_amt\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "print(f\"  –í—Å–µ–≥–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {df.count():,}\")\n",
    "print(f\"  –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö: {int(stats['fraud_count']):,}\")\n",
    "print(f\"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: {stats['customers']:,}\")\n",
    "print(f\"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤: {stats['terminals']:,}\")\n",
    "print(f\"  –°—É–º–º–∞: –æ—Ç {stats['min_amt']:.2f} –¥–æ {stats['max_amt']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6fcff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rows = df.filter(\n",
    "    col(\"tranaction_id\").isNull() |\n",
    "    (col(\"tx_amount\") <= 0) |\n",
    "    (~col(\"tx_fraud\").isin([0, 1]))\n",
    ")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–∏ –≤ Object Storage (–∑–∞–º–µ–Ω–∏—Ç–µ bucket)\n",
    "bad_rows.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"s3a://oa-otus-bucket-b1gfn58hulkq64deu28q/bad_transactions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
