import sys
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, LongType, StringType, DoubleType, IntegerType
)
from pyspark.sql.functions import col, to_timestamp
import argparse


def main():
    parser = argparse.ArgumentParser(description="–û—á–∏—Å—Ç–∫–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∏–∑ S3 ‚Üí Parquet –≤ S3")
    parser.add_argument("--input", required=True,
                        help="–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω—ã–º —Ñ–∞–π–ª–∞–º –≤ S3 (–Ω–∞–ø—Ä–∏–º–µ—Ä, s3a://my-bucket/raw/transactions_*.txt)")
    parser.add_argument("--output", required=True,
                        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è Parquet –≤ S3 (–Ω–∞–ø—Ä–∏–º–µ—Ä, s3a://my-bucket/clean/fraud_data)")
    parser.add_argument("--log-level", default="WARN", choices=["ERROR", "WARN", "INFO", "DEBUG"])

    args = parser.parse_args()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark
    spark = SparkSession.builder \
        .appName("FraudDataCleaning-S3") \
        .getOrCreate()

    spark.sparkContext.setLogLevel(args.log_level)

    # –°—Ö–µ–º–∞ (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –æ–ø–µ—á–∞—Ç–∫–∏ tranaction_id)
    schema = StructType([
        StructField("tranaction_id", StringType(), True),
        StructField("tx_datetime", StringType(), True),
        StructField("customer_id", StringType(), True),
        StructField("terminal_id", StringType(), True),
        StructField("tx_amount", StringType(), True),
        StructField("tx_time_seconds", StringType(), True),
        StructField("tx_time_days", StringType(), True),
        StructField("tx_fraud", StringType(), True),
        StructField("tx_fraud_scenario", StringType(), True)
    ])

    # === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ S3 ===
    print(f"üì• –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {args.input}")
    df_raw = spark.read \
        .option("sep", ",") \
        .option("header", "false") \
        .option("mode", "PERMISSIVE") \
        .option("columnNameOfCorruptRecord", "_corrupt_record") \
        .schema(schema) \
        .csv(args.input)

    # –ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ (–Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å '#')
    if "_corrupt_record" in df_raw.columns:
        # Spark –ø–æ–º–µ—â–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏, –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ö–µ–º–µ, –≤ _corrupt_record
        # –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ ‚Äî —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞ "# ...", –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–∞—Ä—Å—è—Ç—Å—è –∫–∞–∫ 9 –ø–æ–ª–µ–π
        corrupt_count = df_raw.filter(col("_corrupt_record").isNotNull()).count()
        if corrupt_count > 0:
            print(f"üóëÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ {corrupt_count} —Å—Ç—Ä–æ–∫ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ –∏–ª–∏ –æ—à–∏–±–∫–∞–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞.")
        df_raw = df_raw.filter(col("_corrupt_record").isNull()).drop("_corrupt_record")

    # === 2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ ===
    df = df_raw \
        .withColumn("tranaction_id", col("tranaction_id").cast(LongType())) \
        .withColumn("customer_id", col("customer_id").cast(LongType())) \
        .withColumn("terminal_id", col("terminal_id").cast(LongType())) \
        .withColumn("tx_amount", col("tx_amount").cast(DoubleType())) \
        .withColumn("tx_time_seconds", col("tx_time_seconds").cast(LongType())) \
        .withColumn("tx_time_days", col("tx_time_days").cast(LongType())) \
        .withColumn("tx_fraud", col("tx_fraud").cast(IntegerType())) \
        .withColumn("tx_fraud_scenario", col("tx_fraud_scenario").cast(IntegerType())) \
        .withColumn("tx_datetime", to_timestamp(col("tx_datetime"), "yyyy-MM-dd HH:mm:ss"))

    # === 3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å NULL –ø–æ—Å–ª–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è —Ç–∏–ø–æ–≤ ===
    initial_count = df.count()
    df = df.filter(
        col("tranaction_id").isNotNull() &
        col("customer_id").isNotNull() &
        col("terminal_id").isNotNull() &
        col("tx_amount").isNotNull() &
        col("tx_time_seconds").isNotNull() &
        col("tx_time_days").isNotNull() &
        col("tx_fraud").isNotNull() &
        col("tx_fraud_scenario").isNotNull() &
        col("tx_datetime").isNotNull()
    )
    after_type_filter = df.count()
    print(f"üßπ –£–¥–∞–ª–µ–Ω–æ {initial_count - after_type_filter} —Å—Ç—Ä–æ–∫ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏.")

    # === 4. –ë–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª–∞ ===
    df = df.filter(
        (col("tx_amount") > 0) &
        (col("tx_time_seconds") >= 0) &
        (col("tx_time_days") >= 0) &
        (col("tranaction_id") >= 0) &
        (col("customer_id") >= 0) &
        (col("terminal_id") >= 0) &
        (col("tx_fraud").isin([0, 1])) &
        (col("tx_fraud_scenario") >= 0)
    )

    # === 5. –õ–æ–≥–∏–∫–∞: –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ ‚Üí —Å—Ü–µ–Ω–∞—Ä–∏–π > 0 ===
    df = df.filter(
        (col("tx_fraud") == 0) | (col("tx_fraud_scenario") >= 1)
    )

    # === 6. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ tranaction_id ===
    df_clean = df.dropDuplicates(["tranaction_id"])
    final_count = df_clean.count()
    print(f"‚úÖ –ò—Ç–æ–≥–æ–≤–æ–µ —á–∏—Å–ª–æ —Å—Ç—Ä–æ–∫: {final_count}")

    # === 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Parquet –≤ S3 ===
    print(f"üì§ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤: {args.output}")
    df_clean.write \
        .mode("overwrite") \
        .option("compression", "snappy") \
        .parquet(args.output)

    print("‚ú® –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ.")
    spark.stop()


if __name__ == "__main__":
    main()